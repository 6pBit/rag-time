{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search Vector Index Compression\n",
    "This notebook demonstrates different compression configurations for vector search indexes and their impact on storage.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running the notebook, ensure you have the following: \n",
    "\n",
    "- [Fork](https://github.com/microsoft/rag-time/fork) the repository and clone it to your local machine by following the script below:\n",
    "\n",
    "    ```bash\n",
    "    git clone https://github.com/your-org/rag-time.git\n",
    "    cd rag-time\n",
    "    ```\n",
    "\n",
    "- An [Azure account](https://portal.azure.com) with proper permissions to access the following services:\n",
    "    - An **Azure OpenAI** service with an active deployment of a **chat model** and an **embedding model**.\n",
    "    - An **Azure AI Search** service with an index that contains vectorized text data. Follow the instructions in the [Quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-portal-import-vectors?tabs=sample-data-storage%2Cmodel-aoai%2Cconnect-data-storage) to index the documents in [data](./../../data/) folder. \n",
    "- Install Python 3.8 or later from [python.org](https://python.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Use the Notebook\n",
    "\n",
    "### 1. Set Up Environment Variables\n",
    "\n",
    "To store credentials securely, rename `.env.sample` file to `.env` in the same directory as the notebook and update the following variables:\n",
    "\n",
    "```bash\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT=\"https://SEARCHSERVICE.search.windows.net\"\n",
    "AZURE_SEARCH_ADMIN_KEY=\n",
    "AZURE_OPENAI_SERVICE_ENDPOINT=\"https://OPENAISERVICE.openai.azure.com/\"\n",
    "AZURE_OPENAI_EMBED_DEPLOYMENT=\"text-embedding-3-large\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Install Required Libraries\n",
    "\n",
    "Run the first code cell to install the required Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-search-documents==11.6.0b8\n",
    "%pip install azure-identity\n",
    "%pip install datasets\n",
    "%pip install tabulate\n",
    "%pip install python-dotenv\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import Dependencies\n",
    "\n",
    "Run the following command to load environment variables from the `.env` file and import dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    BinaryQuantizationCompression,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    ScalarQuantizationCompression,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    VectorSearchCompressionRescoreStorageMethod,\n",
    "    RescoringOptions\n",
    ")\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Set Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure Search Configuration\n",
    "SERVICE_ENDPOINT = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "credential = DefaultAzureCredential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Example Search Index Configuration\n",
    "\n",
    "Define the example search index with a small vector dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(endpoint=SERVICE_ENDPOINT, credential=credential)\n",
    "\n",
    "index = SearchIndex(\n",
    "    name=\"tinyindex\",\n",
    "    fields=[\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchField(\n",
    "            name=\"embedding\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=3,\n",
    "            vector_search_profile_name=\"embedding_profile\",\n",
    "            stored=False\n",
    "        ),\n",
    "    ],\n",
    "    vector_search=VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"hnsw_config\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(metric=\"cosine\"),\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"embedding_profile\", algorithm_configuration_name=\"hnsw_config\", compression_name=\"binary_compression\"\n",
    "            )\n",
    "        ],\n",
    "        compressions=[\n",
    "            BinaryQuantizationCompression(\n",
    "                compression_name=\"binary_compression\",\n",
    "                rerank_with_original_vectors=None,\n",
    "                default_oversampling=None,\n",
    "                rescoring_options=RescoringOptions(\n",
    "                    enable_rescoring=False,\n",
    "                    rescore_storage_method=VectorSearchCompressionRescoreStorageMethod.DISCARD_ORIGINALS,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    ")\n",
    "\n",
    "index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define Test Scenarios \n",
    "\n",
    "See explanations in reference here:\n",
    "https://learn.microsoft.com/python/api/azure-search-documents/azure.search.documents.indexes.models.searchfield?view=azure-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PREFIX = \"compression-test\"\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"baseline\",\n",
    "        \"compression_type\": None,\n",
    "        \"truncate_dims\": None,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": True,\n",
    "        \"description\": \"Baseline configuration without compression\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline-s\",\n",
    "        \"compression_type\": None,\n",
    "        \"truncate_dims\": None,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Baseline configuration without compression, with stored=False \"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scalar-full\",\n",
    "        \"compression_type\": \"scalar\",\n",
    "        \"truncate_dims\": None,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Scalar quantization (int8) with full dimensions, preserved originals\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scalar-truncated-1024\",\n",
    "        \"compression_type\": \"scalar\",\n",
    "        \"truncate_dims\": 1024,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Scalar quantization (int8) with 1024 dimensions, preserved originals\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"scalar-truncated-1024-discard\",\n",
    "        \"compression_type\": \"scalar\",\n",
    "        \"truncate_dims\": 1024,\n",
    "        \"discard_originals\": True,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Scalar quantization (int8) with 1024 dimensions, discarded originals\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"binary-full\",\n",
    "        \"compression_type\": \"binary\",\n",
    "        \"truncate_dims\": None,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Binary quantization with full dimensions, preserved originals\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"binary-truncated-1024\",\n",
    "        \"compression_type\": \"binary\",\n",
    "        \"truncate_dims\": 1024,\n",
    "        \"discard_originals\": False,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Binary quantization with 1024 dimensions, preserved originals\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"binary-truncated-1024-discard\",\n",
    "        \"compression_type\": \"binary\",\n",
    "        \"truncate_dims\": 1024,\n",
    "        \"discard_originals\": True,\n",
    "        \"stored_embedding\": False,\n",
    "        \"description\": \"Binary quantization with 1024 dimensions, discarded originals\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Execute Index Creation\n",
    "\n",
    "Run the provided code to create vector search indexes with different compression settings. This allows comparing storage efficiency and search quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzureSearchIndexManager:\n",
    "    def __init__(self, service_endpoint: str, credential: str, index_name_prefix: str, vector_dimensions: int):\n",
    "        self.client = SearchIndexClient(endpoint=service_endpoint, credential=credential)\n",
    "        self.index_name_prefix = index_name_prefix\n",
    "        self.vector_dimensions = vector_dimensions\n",
    "\n",
    "    def _create_base_fields(self, stored_embedding=True):\n",
    "        return [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "            SearchField(name=\"title\", type=SearchFieldDataType.String, searchable=True),\n",
    "            SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "            SearchField(\n",
    "                name=\"embedding\",\n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True,\n",
    "                vector_search_dimensions=self.vector_dimensions,\n",
    "                vector_search_profile_name=\"default-profile\",\n",
    "                stored=stored_embedding,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _create_compression_config(\n",
    "        self,\n",
    "        config_type: str,\n",
    "        truncate_dims: int = None,\n",
    "        discard_originals: bool = False,\n",
    "        oversample_ratio: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a compression configuration based on the scenario.\n",
    "        \"\"\"\n",
    "        compression_name = f\"{config_type}-compression\"\n",
    "\n",
    "        # Determine the storage method based on whether originals are discarded\n",
    "        rescore_storage_method = (\n",
    "            VectorSearchCompressionRescoreStorageMethod.DISCARD_ORIGINALS\n",
    "            if discard_originals\n",
    "            else VectorSearchCompressionRescoreStorageMethod.PRESERVE_ORIGINALS\n",
    "        )\n",
    "\n",
    "        # Enable rescoring only if originals are preserved\n",
    "        enable_rescoring = not discard_originals\n",
    "\n",
    "        # Configure rescoring options\n",
    "        rescoring_options = RescoringOptions(\n",
    "            enable_rescoring=enable_rescoring,\n",
    "            default_oversampling=oversample_ratio if enable_rescoring else None,\n",
    "            rescore_storage_method=rescore_storage_method,\n",
    "        )\n",
    "\n",
    "        # Base parameters for compression\n",
    "        base_params = {\n",
    "            \"compression_name\": compression_name,\n",
    "            \"rescoring_options\": rescoring_options,\n",
    "            # Explicitly set deprecated parameters to None\n",
    "            \"rerank_with_original_vectors\": None,\n",
    "            \"default_oversampling\": None,\n",
    "        }\n",
    "\n",
    "        # Add truncation dimension if specified\n",
    "        if truncate_dims:\n",
    "            base_params[\"truncation_dimension\"] = truncate_dims\n",
    "\n",
    "        # Create the appropriate compression object\n",
    "        if config_type == \"scalar\":\n",
    "            compression = ScalarQuantizationCompression(\n",
    "                parameters=ScalarQuantizationParameters(quantized_data_type=\"int8\"),\n",
    "                **base_params,\n",
    "            )\n",
    "        elif config_type == \"binary\":\n",
    "            compression = BinaryQuantizationCompression(\n",
    "                **base_params,\n",
    "            )\n",
    "        else:\n",
    "            compression = None\n",
    "\n",
    "        return compression\n",
    "\n",
    "    def _create_vector_search_config(self, compression_config=None):\n",
    "        \"\"\"\n",
    "        Creates the VectorSearch configuration, including algorithm and compression settings.\n",
    "        \"\"\"\n",
    "        # Define the HNSW algorithm configuration\n",
    "        algorithm_config = HnswAlgorithmConfiguration(\n",
    "            name=\"hnsw-config\",\n",
    "            kind=\"hnsw\",\n",
    "            parameters=HnswParameters(\n",
    "                m=4,\n",
    "                ef_construction=400,\n",
    "                ef_search=500,\n",
    "                metric=\"cosine\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Define the VectorSearchProfile\n",
    "        profiles = [\n",
    "            VectorSearchProfile(\n",
    "                name=\"default-profile\",\n",
    "                algorithm_configuration_name=algorithm_config.name,\n",
    "                compression_name=compression_config.compression_name if compression_config else None,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Assemble the VectorSearch configuration\n",
    "        vector_search = VectorSearch(\n",
    "            profiles=profiles,\n",
    "            algorithms=[algorithm_config],\n",
    "            compressions=[compression_config] if compression_config else None,\n",
    "        )\n",
    "\n",
    "        return vector_search\n",
    "\n",
    "    def create_index(self, scenario: dict):\n",
    "        \"\"\"\n",
    "        Creates or updates an index based on the provided scenario.\n",
    "        \"\"\"\n",
    "        index_name = f\"{self.index_name_prefix}-{scenario['name']}\"\n",
    "\n",
    "        # Use the 'stored_embedding' value from the scenario\n",
    "        stored_embedding = scenario.get('stored_embedding', True)\n",
    "\n",
    "        # Create base fields with the stored_embedding flag\n",
    "        fields = self._create_base_fields(stored_embedding=stored_embedding)\n",
    "\n",
    "        # Create compression configuration if specified\n",
    "        compression_config = None\n",
    "        if scenario[\"compression_type\"]:\n",
    "            compression_config = self._create_compression_config(\n",
    "                config_type=scenario[\"compression_type\"],\n",
    "                truncate_dims=scenario.get(\"truncate_dims\"),\n",
    "                discard_originals=scenario.get(\"discard_originals\", False),\n",
    "            )\n",
    "\n",
    "        # Create vector search configuration\n",
    "        vector_search = self._create_vector_search_config(compression_config)\n",
    "\n",
    "        # Define the SearchIndex\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "        )\n",
    "\n",
    "        # Create or update the index\n",
    "        try:\n",
    "            self.client.create_or_update_index(index)\n",
    "        except ResourceExistsError:\n",
    "            print(f\"Index {index_name} already exists.\")\n",
    "        except Exception as e:\n",
    "            if e.message and \"already exists\" in e.message:\n",
    "                print(f\"Index {index_name} already exists.\")\n",
    "            else:\n",
    "                print(f\"Error creating index {index_name}: {type(e)} - {str(e)}\")\n",
    "    \n",
    "        # Return the index name\n",
    "        return index_name\n",
    "\n",
    "manager = AzureSearchIndexManager(\n",
    "    service_endpoint=SERVICE_ENDPOINT,\n",
    "    credential=credential,\n",
    "    index_name_prefix=INDEX_PREFIX,\n",
    "    vector_dimensions=3072)\n",
    "\n",
    "# Create all index configurations\n",
    "created_indexes = []\n",
    "for scenario in scenarios:\n",
    "    try:\n",
    "        index_name = manager.create_index(scenario)\n",
    "        created_indexes.append({\n",
    "            \"index_name\": index_name,\n",
    "            \"configuration\": scenario[\"description\"]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating index for scenario {scenario['name']}: {str(e)}\")\n",
    "\n",
    "# Display created indexes\n",
    "if len(created_indexes) > 0:\n",
    "    print(\"\\nCreated Indexes:\")\n",
    "    tabulate(created_indexes, headers=\"keys\", tablefmt=\"html\")\n",
    "else:\n",
    "    print(\"\\nNo indexes were created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Load dataset in streaming mode and select only needed columns\n",
    "print(\"Loading dataset in streaming mode...\")\n",
    "ds = load_dataset(\n",
    "    \"Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-1M\",\n",
    "    streaming=True,\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "# Select only needed columns\n",
    "ds = ds.select_columns([\"_id\", \"title\", \"text\", \"text-embedding-3-large-3072-embedding\"])\n",
    "\n",
    "# Take first 100K examples\n",
    "print(\"Taking first 100K examples...\")\n",
    "data = []\n",
    "for i, example in enumerate(ds):\n",
    "    if i >= 100000:\n",
    "        break\n",
    "    data.append(example)\n",
    "\n",
    "# Convert to Arrow table and save to parquet\n",
    "print(\"Converting to parquet...\")\n",
    "output_file = \"dbpedia_100k.parquet\"\n",
    "table = pa.Table.from_pylist(data)  # Using pa.Table instead of pq.Table\n",
    "pq.write_table(\n",
    "    table,\n",
    "    output_file,\n",
    "    compression='snappy'  # Good balance of compression and speed\n",
    ")\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")\n",
    "\n",
    "# Verify the saved file\n",
    "table = pq.read_table(output_file)\n",
    "print(f\"\\nSaved dataset shape: {table.num_rows} rows × {table.num_columns} columns\")\n",
    "print(\"Columns:\", table.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file\n",
    "table = pq.read_table(\"dbpedia_100k.parquet\")\n",
    "\n",
    "# Convert first row to the exact format we need\n",
    "first_doc = {\n",
    "    \"id\": str(table['_id'][0].as_py()),\n",
    "    \"title\": table['title'][0].as_py(),\n",
    "    \"content\": table['text'][0].as_py(),\n",
    "    \"embedding\": table['text-embedding-3-large-3072-embedding'][0].as_py()\n",
    "}\n",
    "\n",
    "# Print info about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Total number of rows: {table.num_rows}\")\n",
    "print(f\"Columns: {table.column_names}\")\n",
    "print(\"\\nFirst document structure:\")\n",
    "print(json.dumps(first_doc, indent=2, default=str))\n",
    "\n",
    "# Print some basic stats about the data\n",
    "print(\"\\nData Statistics:\")\n",
    "print(f\"Embedding dimension: {len(first_doc['embedding'])}\")\n",
    "print(f\"Average title length: {sum(len(str(title)) for title in table['title']) / table.num_rows:.1f} characters\")\n",
    "print(f\"Average text length: {sum(len(str(text)) for text in table['text']) / table.num_rows:.1f} characters\")\n",
    "\n",
    "# Verify there are no null values\n",
    "print(\"\\nChecking for null values:\")\n",
    "for column in table.column_names:\n",
    "    null_count = table[column].null_count\n",
    "    print(f\"{column}: {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def chunk_list(lst: List[Any], chunk_size: int) -> List[List[Any]]:\n",
    "    \"\"\"Split a list into chunks of specified size.\"\"\"\n",
    "    return [lst[i : i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def encode_key(key: str) -> str:\n",
    "    \"\"\"Encode key to be Azure Search compatible using URL-safe base64.\"\"\"\n",
    "    return base64.urlsafe_b64encode(key.encode()).decode()\n",
    "\n",
    "\n",
    "def prepare_documents(table) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert Arrow table to list of documents with base64 encoded IDs.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    total_rows = table.num_rows\n",
    "\n",
    "    print(f\"Converting {total_rows} rows to documents...\")\n",
    "    for i in range(total_rows):\n",
    "        # Encode the ID to make it Azure Search compatible\n",
    "        original_id = str(table[\"_id\"][i].as_py())\n",
    "        encoded_id = encode_key(original_id)\n",
    "\n",
    "        document = {\n",
    "            \"id\": encoded_id,\n",
    "            \"title\": table[\"title\"][i].as_py(),\n",
    "            \"content\": table[\"text\"][i].as_py(),\n",
    "            \"embedding\": table[\"text-embedding-3-large-3072-embedding\"][i].as_py(),\n",
    "        }\n",
    "        documents.append(document)\n",
    "\n",
    "        if i % 1000 == 0:  # Progress indicator\n",
    "            print(f\"Processed {i}/{total_rows} documents...\")\n",
    "\n",
    "    print(\"Document conversion complete\")\n",
    "\n",
    "    # Print first document as sample (including both original and encoded ID)\n",
    "    print(\"\\nSample document format:\")\n",
    "    sample_doc = documents[0].copy()\n",
    "    print(\"Original ID:\", original_id)\n",
    "    print(\"Encoded ID:\", sample_doc[\"id\"])\n",
    "    print(json.dumps(sample_doc, indent=2, default=str))\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def upload_to_search(\n",
    "    documents: List[Dict],\n",
    "    endpoint: str,\n",
    "    index_name: str,\n",
    "    credential,\n",
    "    batch_size: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload documents to Azure Search index using manual batching.\n",
    "    \"\"\"\n",
    "    # Create search client\n",
    "    search_client = SearchClient(\n",
    "        endpoint=endpoint, index_name=index_name, credential=credential\n",
    "    )\n",
    "\n",
    "    total_docs = len(documents)\n",
    "    print(f\"\\nStarting upload to index: {index_name} at {datetime.now()}\")\n",
    "    print(f\"Total documents to upload: {total_docs}\")\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Split documents into batches\n",
    "    batches = chunk_list(documents, batch_size)\n",
    "    total_batches = len(batches)\n",
    "\n",
    "    successful_docs = 0\n",
    "    failed_docs = 0\n",
    "\n",
    "    try:\n",
    "        for batch_num, batch in enumerate(batches, 1):\n",
    "            max_retries = 3\n",
    "            retry_count = 0\n",
    "\n",
    "            while retry_count < max_retries:\n",
    "                try:\n",
    "                    results = search_client.upload_documents(documents=batch)\n",
    "\n",
    "                    # Count successes and failures\n",
    "                    for result in results:\n",
    "                        if result.succeeded:\n",
    "                            successful_docs += 1\n",
    "                        else:\n",
    "                            failed_docs += 1\n",
    "                            print(\n",
    "                                f\"Failed to upload document {result.key}: {result.error}\"\n",
    "                            )\n",
    "\n",
    "                    elapsed_time = datetime.now() - start_time\n",
    "                    print(\n",
    "                        f\"Index {index_name}: Processed batch {batch_num}/{total_batches} \"\n",
    "                        f\"({successful_docs}/{total_docs} docs). \"\n",
    "                        f\"Elapsed time: {elapsed_time}\"\n",
    "                    )\n",
    "\n",
    "                    # Short pause between batches to prevent throttling\n",
    "                    time.sleep(0.25)\n",
    "                    break  # Success - exit retry loop\n",
    "\n",
    "                except Exception as e:\n",
    "                    retry_count += 1\n",
    "                    if retry_count == max_retries:\n",
    "                        print(\n",
    "                            f\"Failed to upload batch after {max_retries} attempts: {str(e)}\"\n",
    "                        )\n",
    "                        failed_docs += len(batch)\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Retry {retry_count}/{max_retries} after error: {str(e)}\"\n",
    "                        )\n",
    "                        time.sleep(2**retry_count)  # Exponential backoff\n",
    "\n",
    "        total_time = datetime.now() - start_time\n",
    "        print(f\"\\nUpload to {index_name} completed:\")\n",
    "        print(f\"Successfully uploaded: {successful_docs} documents\")\n",
    "        print(f\"Failed to upload: {failed_docs} documents\")\n",
    "        print(f\"Total time: {total_time}\")\n",
    "\n",
    "        # Verify final count\n",
    "        result = search_client.search(\"*\", top=0)\n",
    "        final_count = result.get_count()\n",
    "        print(f\"Final document count in index: {final_count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error during upload to {index_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def upload_to_all_indexes(\n",
    "    documents: List[Dict],\n",
    "    endpoint: str,\n",
    "    index_prefix: str,\n",
    "    scenarios: list,\n",
    "    credential: AzureKeyCredential,\n",
    "    batch_size: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload documents to all indexes sequentially.\n",
    "    \"\"\"\n",
    "    total_start_time = datetime.now()\n",
    "\n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        index_name = f\"{index_prefix}-{scenario['name']}\"\n",
    "        print(f\"\\nProcessing index {i} of {len(scenarios)}: {index_name}\")\n",
    "        upload_to_search(\n",
    "            documents=documents,\n",
    "            endpoint=endpoint,\n",
    "            index_name=index_name,\n",
    "            credential=credential,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "    total_time = datetime.now() - total_start_time\n",
    "    print(f\"\\nCompleted all uploads. Total time: {total_time}\")\n",
    "\n",
    "\n",
    "# Usage:\n",
    "print(\"Loading parquet file...\")\n",
    "table = pq.read_table(\"dbpedia_100k.parquet\")\n",
    "print(f\"Loaded {table.num_rows} rows\")\n",
    "\n",
    "# Prepare documents once\n",
    "documents = prepare_documents(table)\n",
    "\n",
    "# Create credential and upload to all indexes\n",
    "upload_to_all_indexes(\n",
    "    documents=documents,\n",
    "    endpoint=SERVICE_ENDPOINT,\n",
    "    index_prefix=INDEX_PREFIX,\n",
    "    scenarios=scenarios,\n",
    "    credential=credential,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from tabulate import tabulate \n",
    "\n",
    "def bytes_to_mb(bytes):\n",
    "    \"\"\"Convert bytes to megabytes with 4 decimal places\"\"\"\n",
    "    return round(bytes / (1024 * 1024), 4)\n",
    "\n",
    "def get_index_sizes(\n",
    "    endpoint: str,\n",
    "    index_prefix: str,\n",
    "    scenarios: list,\n",
    "    credential,\n",
    "    retry_attempts: int = 3\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Get and print storage sizes for all indexes, with retry logic for eventual consistency.\n",
    "    \"\"\"\n",
    "    # Create search index client\n",
    "    search_index_client = SearchIndexClient(endpoint=endpoint, credential=credential)\n",
    "    \n",
    "    print(\"\\nGathering index statistics...\")\n",
    "    print(\"Note: There may be delays in finding index statistics after document upload\")\n",
    "    print(\"Index statistics is not a real-time API\\n\")\n",
    "    \n",
    "    # Collect all index sizes with retries\n",
    "    index_data = []\n",
    "    for scenario in scenarios:\n",
    "        index_name = f\"{index_prefix}-{scenario['name']}\"\n",
    "        \n",
    "        for attempt in range(retry_attempts):\n",
    "            try:\n",
    "                stats = search_index_client.get_index_statistics(index_name)\n",
    "                storage_size = bytes_to_mb(stats[\"storage_size\"])\n",
    "                vector_size = bytes_to_mb(stats[\"vector_index_size\"])\n",
    "                total_size = storage_size + vector_size\n",
    "                index_data.append({\n",
    "                    'Index Name': index_name,\n",
    "                    'Scenario': scenario['name'],\n",
    "                    'Storage Size (MB)': storage_size,\n",
    "                    'Vector Size (MB)': vector_size,\n",
    "                    'Total Size (MB)': total_size,  # Added for sorting\n",
    "                })\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if attempt == retry_attempts - 1:\n",
    "                    print(f\"Failed to get statistics for {index_name} after {retry_attempts} attempts: {str(e)}\")\n",
    "                else:\n",
    "                    print(f\"Retry {attempt + 1}/{retry_attempts} for {index_name}\")\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    # Find baseline storage and vector sizes\n",
    "    baseline_entry = next((entry for entry in index_data if entry['Scenario'] == 'baseline'), None)\n",
    "    if not baseline_entry:\n",
    "        print(\"Baseline scenario not found.\")\n",
    "        return\n",
    "    baseline_storage_size = baseline_entry['Storage Size (MB)']\n",
    "    baseline_vector_size = baseline_entry['Vector Size (MB)']\n",
    "    \n",
    "    # Compute reduction percentages compared to baseline\n",
    "    for entry in index_data:\n",
    "        storage_reduction_pct = ((baseline_storage_size - entry['Storage Size (MB)']) / baseline_storage_size) * 100\n",
    "        vector_reduction_pct = ((baseline_vector_size - entry['Vector Size (MB)']) / baseline_vector_size) * 100\n",
    "        entry['Storage Reduction (%)'] = f\"{storage_reduction_pct:.2f}\"\n",
    "        entry['Vector Reduction (%)'] = f\"{vector_reduction_pct:.2f}\"\n",
    "    \n",
    "    # Sort by total size\n",
    "    index_data.sort(key=lambda x: x['Total Size (MB)'], reverse=True)\n",
    "    \n",
    "    # Prepare table headers and rows\n",
    "    headers = [\n",
    "        'Index Name', 'Scenario', 'Storage Size (MB)', 'Storage Reduction (%)',\n",
    "        'Vector Size (MB)', 'Vector Reduction (%)'\n",
    "    ]\n",
    "    table_rows = [\n",
    "        [\n",
    "            entry['Index Name'],\n",
    "            entry['Scenario'],\n",
    "            f\"{entry['Storage Size (MB)']:.4f}\",\n",
    "            entry['Storage Reduction (%)'],\n",
    "            f\"{entry['Vector Size (MB)']:.4f}\",\n",
    "            entry['Vector Reduction (%)']\n",
    "        ]\n",
    "        for entry in index_data\n",
    "    ]\n",
    "    \n",
    "    # Print the table using tabulate\n",
    "    return tabulate(table_rows, headers=headers, tablefmt=\"html\")\n",
    "    \n",
    "\n",
    "# Get and print index sizes\n",
    "get_index_sizes(\n",
    "    endpoint=SERVICE_ENDPOINT,\n",
    "    index_prefix=INDEX_PREFIX,\n",
    "    scenarios=scenarios,\n",
    "    credential=credential\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Search quality\n",
    "\n",
    "Let's test the search quality of the compressed indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import azure.identity\n",
    "import openai\n",
    "\n",
    "\n",
    "token_provider = azure.identity.get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_version=\"2023-07-01-preview\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_SERVICE_ENDPOINT\"],\n",
    "    azure_ad_token_provider=token_provider)\n",
    "\n",
    "def get_embedding(text):\n",
    "    get_embeddings_response = openai_client.embeddings.create(model=os.environ[\"AZURE_OPENAI_EMBED_DEPLOYMENT\"], input=text)\n",
    "    return get_embeddings_response.data[0].embedding\n",
    "\n",
    "results = {}\n",
    "for scenario in scenarios:\n",
    "    results[scenario['name']] = []\n",
    "    index_name = f\"{INDEX_PREFIX}-{scenario['name']}\"\n",
    "    search_client = SearchClient(SERVICE_ENDPOINT, index_name, credential=credential)\n",
    "    search_query = \"first avian dinosaur in the fossil record\"\n",
    "    search_vector = get_embedding(search_query)\n",
    "    r = search_client.search(\n",
    "            top=5, \n",
    "            vector_queries=[\n",
    "                    VectorizedQuery(vector=search_vector, k_nearest_neighbors=50, fields=\"embedding\")])\n",
    "    for doc in r:\n",
    "        results[scenario['name']].append(doc[\"title\"])\n",
    "\n",
    "rows = []\n",
    "headers = [scenario['name'] for scenario in scenarios]\n",
    "for i in range(5):\n",
    "    rows.append([results[scenario['name']][i] for scenario in scenarios])\n",
    "tabulate(rows, headers=headers, tablefmt=\"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "- **Environment Variables Not Loaded:** Ensure you have correctly set the .env file or manually export them in your terminal before running the notebook\n",
    "- **Authentication Issues:** If using Managed Identity, make sure your Azure identity has proper role assignments.\n",
    "- **Dataset Not Found:** Ensure that the dataset is correctly downloaded and converted into Parquet format.\n",
    "- **Index Creation Errors:** Check that your Azure AI Search service is configured correctly and has the required storage capacity.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates different vector search compression techniques in Azure AI Search, including scalar and binary quantization. It evaluates the impact of compression on storage efficiency and retrieval quality, helping optimize large-scale AI search applications.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
